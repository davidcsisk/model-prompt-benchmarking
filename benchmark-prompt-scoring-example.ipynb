{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "201307bf",
   "metadata": {},
   "source": [
    "## Benchmark Prompt Scoring Example\n",
    "6/12/2025, Dave Sisk, https://github.com/davidcsisk, https://www.linkedin.com/in/davesisk-doctordatabase/\n",
    "\n",
    "#### Scoring Approaches\n",
    "\n",
    "The following scoring approaches are used to evaluate the similarity between prompts and responses:\n",
    "\n",
    "1. **Cosine Similarity**:\n",
    "   - **Description**: Measures the cosine of the angle between two vectors in an embedding space. It evaluates the semantic similarity between the prompt and response.\n",
    "   - **Range**: 0 to 1\n",
    "   - **Better Score**: Higher scores indicate greater similarity.\n",
    "\n",
    "2. **BERTScore (F1)**:\n",
    "   - **Description**: Uses contextual embeddings from BERT to compute precision, recall, and F1 scores for text similarity. The F1 score is used here.\n",
    "   - **Range**: 0 to 1\n",
    "   - **Better Score**: Higher scores indicate better alignment between the prompt and response.\n",
    "\n",
    "3. **ROUGE-L**:\n",
    "   - **Description**: Measures the longest common subsequence (LCS) between the prompt and response. It is commonly used for evaluating text summarization.\n",
    "   - **Range**: 0 to 1\n",
    "   - **Better Score**: Higher scores indicate better overlap between the prompt and response.\n",
    "\n",
    "4. **BLEU**:\n",
    "   - **Description**: Evaluates the n-gram overlap between the prompt and response. It is commonly used for machine translation tasks.\n",
    "   - **Range**: 0 to 1\n",
    "   - **Better Score**: Higher scores indicate better n-gram overlap.\n",
    "\n",
    "Each of these metrics provides a unique perspective on the quality of the response, with higher scores generally indicating better performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f0ca3f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies (safe to re-run)\n",
    "#!pip install pandas sentence-transformers bert_score nltk rouge-score --quiet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dc4a1881",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dave Sisk\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:11: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from bert_score import score as bert_score\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from rouge_score import rouge_scorer\n",
    "from IPython.display import display\n",
    "from pathlib import Path\n",
    "\n",
    "# Download NLTK resources\n",
    "import nltk\n",
    "nltk.download('punkt', quiet=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cc8cac9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scoring in progress...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Scores written to: benchmark-prompt-scoring_chatgpt-test_scored.csv\n"
     ]
    }
   ],
   "source": [
    "# Score the responses in a variety of ways into a new CSV file \n",
    "def process_file(file_path):\n",
    "    \"\"\"Process file and save scores without any UI interaction\"\"\"\n",
    "    # Load file\n",
    "    df = pd.read_csv(file_path)\n",
    "    assert \"Prompt\" in df.columns and \"Response\" in df.columns, \"❌ CSV must contain 'Prompt' and 'Response' columns\"\n",
    "\n",
    "    # Load models\n",
    "    embed_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "    rouge = rouge_scorer.RougeScorer([\"rougeL\"], use_stemmer=True)\n",
    "    smooth = SmoothingFunction()\n",
    "\n",
    "    cosine_scores = []\n",
    "    bert_f1_scores = []\n",
    "    rouge_l_scores = []\n",
    "    bleu_scores = []\n",
    "\n",
    "    print(\"Scoring in progress...\")\n",
    "\n",
    "    for idx, row in df.iterrows():\n",
    "        prompt, response = row[\"Prompt\"], row[\"Response\"]\n",
    "\n",
    "        # Cosine similarity (embeddings)\n",
    "        try:\n",
    "            p_vec = embed_model.encode(prompt, convert_to_tensor=True)\n",
    "            r_vec = embed_model.encode(response, convert_to_tensor=True)\n",
    "            cosine = util.pytorch_cos_sim(p_vec, r_vec).item()\n",
    "        except:\n",
    "            cosine = 0.0\n",
    "\n",
    "        # BERTScore\n",
    "        try:\n",
    "            _, _, F1 = bert_score([response], [prompt], lang=\"en\", verbose=False)\n",
    "            bert_f1 = F1[0].item()\n",
    "        except:\n",
    "            bert_f1 = 0.0\n",
    "\n",
    "        # ROUGE-L\n",
    "        try:\n",
    "            rouge_l = rouge.score(prompt, response)[\"rougeL\"].fmeasure\n",
    "        except:\n",
    "            rouge_l = 0.0\n",
    "\n",
    "        # BLEU\n",
    "        try:\n",
    "            prompt_tokens = nltk.word_tokenize(prompt)\n",
    "            response_tokens = nltk.word_tokenize(response)\n",
    "            bleu = sentence_bleu([prompt_tokens], response_tokens, smoothing_function=smooth.method1)\n",
    "        except:\n",
    "            bleu = 0.0\n",
    "\n",
    "        cosine_scores.append(cosine)\n",
    "        bert_f1_scores.append(bert_f1)\n",
    "        rouge_l_scores.append(rouge_l)\n",
    "        bleu_scores.append(bleu)\n",
    "\n",
    "    df[\"CosineSimilarity\"] = cosine_scores\n",
    "    df[\"BERTScore_F1\"] = bert_f1_scores\n",
    "    df[\"ROUGE_L\"] = rouge_l_scores\n",
    "    df[\"BLEU\"] = bleu_scores\n",
    "\n",
    "    # Save results to file\n",
    "    output_file = Path(file_path).stem + \"_scored.csv\"\n",
    "    df.to_csv(output_file, index=False)\n",
    "    print(f\"✅ Scores written to: {output_file}\")\n",
    "\n",
    "# Example usage (uncomment to run):\n",
    "process_file(\"benchmark-prompt-scoring_chatgpt-test.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e681a6f",
   "metadata": {},
   "source": [
    "Open the scored CSV file note above with a suitable spreadsheet program or CSV viewer/editor to examine the scores. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
